######################## PBM Configuration ############################

## This file is a full configuration example documenting all 
## options in comments. 
##
## You can find the full configuration reference here:
## https://docs.percona.com/percona-backup-mongodb/configuration-options.html

#======================Storage Configuration============================

#storage:

##  Remote backup storage type. Supported types: S3, GCS, shared filesystem, Azure
 

#---------------------S3 Storage Configuration--------------------------
#  type:
#    s3:

## Specify the location and name of the bucket that you have configured on the S3 
#     region: 
#     bucket: 

## The data directory to store backups in. 
## When undefined, backups are saved at the root of the bucket.
#     prefix:  

## An optional custom URL to access the bucket. Useful for S3-compatible storage (e.g. MinIO)
#     endpointURL: 

## S3 access credentials.
#     credentials:
#       access-key-id: 
#       secret-access-key:
#       session-token:  

## The size of data chinks (in MB) to upload to the bucket.
#     uploadPartSize: 10

## Data upload configuration
#     maxUploadParts: 10,000

## Set the storage classes for data objects in the bucket. 
## If undefined, the default STANDARD object will be used.
#     storageClass:  

## Allow PBM to upload data to storage with self-issued TLS certificates. 
## Use it with caution as it might leave a hole for man-in-the-middle attacks. 
#     insecureSkipTLSVerify:

## Debug level logging configuration for S3 requests.
#     debugLogLevels: 

## Server-side encryption options.
#     serverSideEncryption:
#       sseAlgorithm: aws:kms
#       kmsKeyID: 
## Options for server-side encryption with customer-provided keys stored 
## on the client side
#       sseCustomerAlgorithm: AES256
#       sseCustomerKey: 
 
## Retry upload configuration options.
#     retryer:
#       numMaxRetries: 3
#       minRetryDelay: 30
#       maxRetryDelay: 5

#--------------------Filesystem Configuration---------------------------
#  type:
#    filesystem:

## The path to backup directory
#      path: 

#--------------------Microsoft Azure Configuration-----------------------
#  type:
#    azure:

## Azure storage account and container name
#      account: 
#      container: 

## Where to store data in the container
#      prefix: 

## Specify the access key
#      credentials:
#        key: 

#--------------------Google Cloud Storage Configuration-----------------------
#  type:
#    gcs:

## Specify the name of the bucket that you have configured in GCS 
#     bucket: 

## The data directory to store backups in. 
## When undefined, backups are saved at the root of the bucket.
#     prefix:  

## GCS access credentials. You can use either HMAC or Service Account keys.
## HMAC Example:
#     credentials:
#       hmacAccessKey: GOOG1E7GHK2...
#       hmacSecret: U7PCRkLy...

## Service Account Example:
#     credentials:
#       clientEmail: account@dev-sandbox.iam.gserviceaccount.com
#       privateKey: "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBg...kqh3vNY2NQ=\n-----END PRIVATE KEY-----\n"
#

#====================Point-in-Time Recovery Configuration==================

#pitr:
#  enabled: false

## Adjust the size op oplog chunks (in minutes).
#  oplogSpanMin: 10 

## Set a compression method and level
#  compression:
#  compressionLevel:

## Save oplog slicing without the base backup
#  oplogOnly: false

#==========================Backup Configuration============================

## Adjust priority of mongod nodes for making backups. The highest priority 
## node is making a backup.
## Nodes with the same priority are randomly elected for a backup.
#backup:
#  priority:

## Set a compression method and level
#  compression:
#  compressionLevel:

#==========================Restore Configuration===========================

## Options to adjust the memory consumption in environments with tight memory bounds.
#restore:
#  batchSize: 500
#  numInsertionWorkers: 10
#  numParallelCollections: 

## Adjust concurrent download of data chunks from storage for physical restore.
#  numDownloadWorkers: 
#  maxDownloadBufferMb: 
#  downloadChunkMb: 32

## Specify the custom path to the mongod binaries for the entire deployment/
# individual nodes for database restarts during physical restore
#  mongodLocation: 
#  mongodLocationMap:
#    "node-name:port":"path"
